---
title: "Natural Language Processing and the xDeepDive Architecture for Article Recommendation"
format:
    pdf:
        toc: true
abstract:
    Data repositories that rely on users to submit data will often have data holdings that are biased towards individuals who know about the data repository. Inequity in academic resources, and in knowledge networks within academia can mean that data resources that are open, may be magnifying these inequities since not all researchers will have access to the data repository. To help address this inequity we propose a service that can use article metadata to identify articles that may be well suited to the Neotoma Paleoecology Database. This will allow data curators to reach out to individual authors to solicit their contributions, acting as a form of outreach for the database, and helping to build a larger community of contributors to improve the breadth of data holdings within the database.
author:
    - name: Simon J Goring
      orcid: 0000-0002-2700-4605
      email: goring@wisc.edu
      affiliations:
        - name: University of Wisconsin - Madison; Department of Geography
          address: 550 N Park St
          city: Madison
          state: WI
          country: United States of America
          postal-code: 53706
        - name: University of Wisconsin - Madison; Center for Climatic Research
          address: 1225 W Dayton St
          city: Madison
          state: WI
          country: United States of America
          postal-code: 53706
    - name: Socorro Dominguez
      email: sedv8808@ht-data.com
      orcid: 0000-0002-7926-4935
      affiliations:
        - name: HT Data
          city: Vancouver
          state: BC
          country: Canada
    - name: Kelly Wu
    - name: Ty Andrews
    - name: Jenit Jain
    - name: Shaun Hutchinson
lang: en
bibliography: bibliography.bib
keywords:
    - natural language processing
    - Neotoma Paleoecology Database
    - xDeepDive
---

```{r loadPackages, echo=FALSE, message=FALSE}
source('src/load_packages.R')
```

# Introduction

Community Curated Data Resources play an important role in managing and providing data to disciplinary research communities. In particular CCDRs such as Neotoma act as a nexus for education, research, outreach and community by serving as a focal point for the broader community [@goring2018nexus]. Data representivity can be a considerable challenge for CCDRs as a result of bias in data contributors, or the practices and global distributions of researchers and research projects [@zhao2019international;@queenan2016representativeness;@PROENCA2017256;@minor2016safeguarding]. Biases in data representivity can be addressed in several way. Organizations such as the International Tree Ring Database have used statistical tools to identify high priority areas for future study [@zhao2019international]. This approach provides high level guidance to existing users of the data resource, providing them with an authority to use in requesting research funding and planning future data collection campaigns. Medical databases can use population-level resources such as census data to identify shortcomings within their data resources [@queenan2016representativeness], prompting calls for additional recruitment of data contributors with patient-bases that are more representative of the overall population. In general these two approaches lead us to either targeting existing data (or ongoing data collection efforts) or targeting new data collection efforts through advocay.

The Neotoma Paleoecology Database is a global data resource supporting paleoecological research [@williams2018neotoma]. Neotoma is a database of databases, representing many distinct user communities and data collection efforts, sharing a common data model and leadership structure. Neotoma represents data contributions from XXXX researchers from around the globe, however, Neotoma relies entirely on user contributions, and as such is susceptible to data representivity bias.

One approach to addressing representation bias is to proactively request data contributions from researchers when articles that are well suited for inclusion in the database appear in the literature. A challenge with this approach is that it requires one or more individuals to be attentive to article alerts across a number of potential search terms. Neotoma contains data from at least 25 different data-types, including pollen, diatoms, water chemistry and vertebrate fossils, however, not all papers published on these subjects is suited to Neotoma. This means that there would be considerable effort required to search, filter and request data from the appropriate papers.

Advances in machine learning and new sources of digital data point to a way forward using a data science approach. Leveraging full text searching and rich metadata it should be possible to provide an inclusive set of search terms and allow a machine learning algorithm to predict article relevance. Here we present such an approach. Using the GeoDeepDive server we search for all articles published within a certain time period with a set of known terms. We augment the article metadata with additional metadata from CrossRef to provide a set of data on which a predictive model can be applioed. A predictive model is then applied to the set of article metadata to indicate relevance along a 0 - 1 score. These articles can then be easily assessed and evaluated for inclusion within the database.

# Methodology

We're using NLP tools and a machine learning algorithm to identify suitability for a paper within Neotoma. From this we will then extract metadata to create a default object. The approach described here uses human curation, and data from the Neotoma Paleoecology Database to build a set of publication metadata that can be used to train a Machine Learning algorithm to predict article suitability for the database. Using commercial cloud computing services and public APIs we query data to add to the list of suitable and non-suitable articles for the database, and, with additional manual curation, we re-train the model to (ideally) improve model outputs in the long-term.

# Algorithm and Implementation

## Article relevance training

![**Figure 1**. *Source and paths of primary data for the article relevance model. This pathway is scripted to support an itterative approach that allows us to rebuild the primary data object to be used for training. The dataset would be rebuilt as new articles are added to Neotoma, or if new blocks of articles are reviewed by a data steward or other researcher who hopes to discover new articles within a particular domain.*](img/MetaReview.png)

We downloaded $n$ papers from PubMed using the pubmed API and a set of keywords that would be likely to include articles relevant to Neotoma, but also articles without direct relevance ("pollen", "archaeology", "'stone age'", "aerobiology", "allergies", "mastodon", "diatoms", "paleoecology", "space", "diatom AND paleoecology", "ostracode", "high resolution sediment"). We supplemented this list of articles with all articles within Neotoma that had an accompanying DOI. All articles were then hand-tagged (by SJG) using SMART () as either "Neotoma", "Not Neotoma" and "Maybe Neotoma". We used the "Maybe Neotoma" tag for articles that were likely to be of interest to the Neotoma Data Steward community, but were unlikely to be entered into Neotoma because the primary disciplinary community likely had a different data repository of record. For example, high resolution tephrachronology is critical for chronology construction within Neotoma, but the primary repository of record is likely EarthChem.

From the tagged articles we extracted metadata from CrossRef and PubMed to provide a more complete data object. This metadata excluded the use of the article fulltext since this would not be available for many legacy publications. For each model we apply an 80/20 trining-testing split to the data to limit the possibility of overfitting. A set of baseline models were then constructed using SciKitLearn in Python.

All articles, with associated metadata were then stored in a Parquet formatted file in an AWS S3 bucket in the cloud. Each article included all associated metadata and the additional columns "relevant", "...".

Model building used nine models implemented from the Python ScikitLearn package [@scikit-learn], along with a dummy model against which to compare models. For each model we assessed the time required to fit the model and to predict results on the testing dataset, recall for the test and training sets, F1 scores for the test and training sets, as well as precision and accuracy for the datasets.

The models are trained on the article subject (as defined by the publisher and reported by CrossRef), and a "bag of words" representation of the concatenated article title and abstract. We do not report "journal" because we want to be able to report the relevance of articles in new (or old) journals that are not represented in the training dataset, particularly if these journals represent regional or domain specific journals that represent underserved communities. We use "Subject" as a proxy for journal focus or content. We concatenate article title and abstract to capture greater textual information about the articles. Additionally, metadata reporting by publishers to CrossRef is variable. We almost always get an article title, but, for particular journals, abstract is often excluded.

## Data Preprocessing

Both the Subject column and the Title/Abstract concatenation are pre-processed using `CountVectorizer()` to generate a sparse matrix of terms associated with each entry. We limit the maximum number of terms to 1000, remove english stopwords and remove accents using unicode mapping. In addition, the default parameters for `CountVectorzer()` transform all terms to lowercase.

## Model Selection and Tuning

From the initial model training exercise 

# Results

## Article Tagging

### Training Data

```{r, generateStats, echo = FALSE, warnings = FALSE}
source('src/basic_neotoma_stats.R')
```

Neotoma maintains an internal list of publications associated with the database datasets. The `r overall_stats$datasets` datasets within Neotoma are associated with `r overall_stats$publications` publications, spanning the last $y$ years. Of the records within Neotoma, many come from grey literature, or were added to the database before the wide use of DOIs for articles. As a result there is an element of incomplete data across the records. For this task, we limited the articles to only those with a recorded DOI in the database. This resulted in the inclusion of $DOI$ articles from Neotoma. From there, an additional XXX articles were obtained and tagged to improve the model. These were tagged using the SMART [@chew2019smart] annotation tool, with classes "Neotoma", "Not Neotoma" and "Maybe Neotoma".

The composite dataset is highly imbalanced. Of the $n$ articles tagged (or already within Neotoma), only $n_{2}$ articles were classed as being appropriate for Neotoma, with $n_3$ identified as being "Maybe Neotoma" and $n_{4}$ were in the class "Not Neotoma". This reflects the fact that paleoecology papers that include primary data represent only a small proportion of all articles published, even when subject and keyword searches focus on terms suited to the discipline.

## Model Building

Model building used nine models implemented from the Python ScikitLearn package [@scikit-learn], along with a dummy model against which to compare models. For each model we assessed the time required to fit the model and to predict results on the testing dataset, recall for the test and training sets, F1 scores for the test and training sets, as well as precision and accuracy for the datasets.

**Table X**. *Models used for binary ('relevant', 'not relevant') relevance fitting. Each model was run with a `random_state` defined, to ensure reproducibility of results, but the state is not defined in this table, to save space.*

| Model | Model Class | Function Call |
| ----- | ----------- | ------------- |
| Dummy | Null | DummyClassifier() |
| Logistic Regression | Linear Model | LogisticRegression(class_weight="balanced", max_iter=1000, ) |
| Decision Tree | Decision Tree | DecisionTreeClassifier(class_weight="balanced", max_depth=200) |
| kNN | Nearest Neighbours | KNeighborsClassifier() |
| Naive Bayes | Naive Bayes | BernoulliNB() |
| RBF SVM | Support Vector Machine | SVC(class_weight="balanced") |
| RF | Ensemble Methods | RandomForestClassifier(class_weight="balanced") |
| LGBM | Gradient Boosting | LGBMClassifier(class_weight="balanced") |
| CatBoost | Gradient Boosting | CatBoostClassifier(verbose=0) |
| XGBoost | Gradient Boosting | XGBClassifier(class_weight="balanced", verbosity=0) |

## Article Relevance

The final model was $x$% accurate, identifying stuff? Feature importance . . .

TODO: Test representivity by comparing the distribution of journal articles by journal; between recommended and the current distribution in Neotoma (?)

# Conclusions


# Acknowledgements


# Code availability Section

 * Contact: goring@wisc.edu
 * Hardware requirements: IBM PC 8086 Processor with 1MB RAM and two 1.44MB Floppy Disks.
 * Program language: Python, R
 * Software required: None
 * Program size: ...

The source codes are available for downloading at the link:
https://github.com/NeotomaDB/MetaExtractor

# References